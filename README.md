# Telecom Customer Churn Prediction using Machine Learning

An end-to-end machine learning project that predicts customer churn in the telecommunications industry using Random Forest classification.

## ğŸ“‹ Project Overview

Customer churn prediction is a critical business problem in the telecom industry. This project implements a complete ML pipeline to identify at-risk customers, enabling proactive retention strategies.

**Key Highlights:**
- 79% prediction accuracy with 82% ROC-AUC score
- Handles class imbalance using SMOTE technique
- Provides interpretable feature importance for business insights
- Production-ready code with proper preprocessing and validation

## ğŸ¯ Business Impact

- **Cost Savings**: Customer acquisition costs 5-25x more than retention
- **Revenue Protection**: 5% retention increase can boost profits by 25-95%
- **Targeted Marketing**: Focus resources on high-risk customer segments
- **Proactive Strategy**: Shift from reactive to predictive customer management

## ğŸ› ï¸ Technologies Used

- **Python 3.7+**
- **Data Processing**: Pandas, NumPy
- **Machine Learning**: Scikit-learn, imbalanced-learn
- **Visualization**: Matplotlib, Seaborn
- **Model**: Random Forest Classifier

## ğŸ“Š Dataset

**Source:** [Telco Customer Churn Dataset (IBM)](https://www.kaggle.com/datasets/yeanzc/telco-customer-churn-ibm-dataset) - Kaggle

**Size:** 7,043 customer records with 33 features

**Features:**
- Demographics (gender, senior citizen status, dependents)
- Account information (tenure, contract type, payment method)
- Service subscriptions (phone, internet, streaming services)
- Billing information (monthly charges, total charges)

## ğŸš€ Getting Started

### Prerequisites

Python 3.7 or higher is required.

### Installation

1. Clone this repository:
   ```bash
   git clone <your-repo-url>
   cd <repo-name>
   ```

2. Install required packages:
   ```bash
   pip install -r requirements.txt
   ```

### Usage

1. Download the dataset from [Kaggle](https://www.kaggle.com/datasets/yeanzc/telco-customer-churn-ibm-dataset) and place the `Telco-Customer-Churn.csv` file in the `data/` directory (or root directory)
2. Open and run the Jupyter notebook:
   ```bash
   jupyter notebook telecom_churn_prediction.ipynb
   ```
3. Follow the notebook cells sequentially to reproduce the analysis

## ğŸ“ˆ Model Performance

| Metric | Score |
|--------|-------|
| Accuracy | 0.79 |
| Precision | 0.60 |
| Recall | 0.59 |
| F1-Score | 0.59 |
| ROC-AUC | 0.82 |

**Confusion Matrix Results:**
- True Negatives: 885
- False Positives: 300
- False Negatives: 154
- True Positives: 222

## ğŸ” Key Features

### 1. Comprehensive Data Pipeline
- Data cleaning and type conversion
- Feature engineering (tenure grouping)
- One-hot encoding for categorical variables
- Stratified train-test split

### 2. Class Imbalance Handling
- SMOTE (Synthetic Minority Over-sampling Technique)
- Balanced training data while preserving test set integrity
- 2,635 synthetic samples generated

### 3. Model Optimization
- Random Forest with 100 trees
- Regularization to prevent overfitting
- Feature standardization
- Proper cross-validation

### 4. Interpretability
- Feature importance analysis
- Confusion matrix visualization
- Business-friendly metrics

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ telecom_churn_prediction.ipynb         # Main analysis notebook
â”œâ”€â”€ data/                                  
â”‚   â””â”€â”€ Telco-Customer-Churn.csv           # Dataset (add your own)
â”œâ”€â”€ requirements.txt                       # Python dependencies
â”œâ”€â”€ .gitignore                             # Git ignore rules
â””â”€â”€ README.md                              # Project documentation
```

## ğŸ”® Future Improvements

- [ ] Hyperparameter tuning with GridSearchCV
- [ ] Experiment with ensemble methods (XGBoost, LightGBM)
- [ ] Implement cost-sensitive learning
- [ ] Create REST API for real-time predictions
- [ ] Deploy model using Flask/FastAPI
- [ ] Add monitoring and retraining pipeline

## ğŸ“ Key Takeaways

1. **Data Quality Matters**: Proper preprocessing and feature engineering significantly impact model performance
2. **Balance is Critical**: SMOTE effectively addresses class imbalance in churn prediction
3. **Interpretability vs Performance**: Random Forest offers a sweet spot between accuracy and explainability
4. **Business Context**: Understanding domain-specific challenges (like acquisition costs) guides model design
